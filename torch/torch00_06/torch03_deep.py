############################################################
#0. ì¤€ë¹„
############################################################
import torch.optim as optim
import torch.nn as nn
import numpy as np
import torch
import random

RS = 555
torch.manual_seed(RS)
np.random.seed(RS)
random.seed(RS)

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device('cuda' if USE_CUDA else 'CPU' )

print('torch :', torch.__version__, 'devise :', DEVICE)
# torch : 2.7.1+cu126 devise : cuda
# tensorì—ì„œëŠ” ê°€ìƒí™˜ê²½ì— GPU ìš©ìœ¼ë¡œ ê¹”ë©´ ë°”ë¡œ ì‚¬ìš©
# torchì—ì„œëŠ” ë‘ ê°€ì§€ ì„¤ì •ì„ ë§ì¶°ì•¼ì§€ GPU ì‚¬ìš©

############################################################
#1. ë°ì´í„°
############################################################

x = np.array([1, 2, 3])
y = np.array([1, 2, 3])

x = torch.FloatTensor(x)
""" print(x)        : tensor([1., 2., 3.]) """
""" print(x.shape)  : torch.Size([3]) """
""" print(x.size()) : torch.Size([3]) """

# torchëŠ” ìµœì†Œ matrix í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬ (x, y ëª¨ë‘)
x = x.unsqueeze(1).to(DEVICE)

                    # unsqueeze(n) = flatten ë°˜ëŒ€ ê°™ì€ ëŠë‚Œ, n ë²ˆì§¸ ì°¨ì›ì„ ì¶”ê°€
""" print(x)        : tensor([[1.],
                              [2.],
                              [3.]]) """
""" print(x.shape)  : torch.Size([3, 1]) """
""" print(x.size()) : torch.Size([3, 1]) """

y = torch.FloatTensor(y).unsqueeze(1).to(DEVICE)
""" print(y.size()) : torch.Size([3, 1]) """
mean = torch.mean(x)
std = torch.std(x)
############### Scaler ###############
x = (x - mean) / std
######################################
print('ìŠ¤ì¼€ì¼ë§ í›„ :', x)

# exit()
# ë°ì´í„°ë¥¼ GPU ìš©ìœ¼ë¡œ ì„¤ì •

############################################################
#2. ëª¨ë¸
############################################################
model = nn.Sequential(
    nn.Linear(1, 5),
    nn.Linear(5, 4),
    nn.Linear(4, 3),
    nn.Linear(3, 2),
    nn.Linear(2, 1)
).to(DEVICE)

                                  # nn : neural network
                                  # nn.Linear(input, output)
                                  # linear : y = wx + b / ì‚¬ì‹¤ tensorflowëŠ” y = xw + b
                                                        # í–‰ë ¬ ì—°ì‚°ì˜ ê¸°ë³¸ : (ì•ì˜ í–‰) * (ë’¤ì˜ ì—´)
                                                        # xì˜ ì»¬ëŸ¼(í–‰)ì— ë§ì¶°ì„œ ê°€ì¤‘ì¹˜ì˜ ì—´ì„ ìƒì„±
''' in tensorflow...
    model = Sequential()
    model.add(Dense(1, input_dim = 1)) '''

############################################################
#3. ì»´íŒŒì¼ í›ˆë ¨
############################################################ 
criterion = nn.MSELoss()        #  criterion : í‘œì¤€
# optimizer = optim.Adam(model.parameters(), lr = 0.01)
optimizer = optim.SGD(model.parameters(), lr = 0.002)
                # SGD, Stochastic Gradient Descent :í†µê³„ì ì¸ ê²½ì‚¬ í•˜ê°•ë²•

def train(model, criterion, optimizer, x, y):
    # model.train()                     # [í›ˆë ¨ëª¨ë“œ] Default : DROPOUT, BATCHNORMAILATION ì ìš©

    optimizer.zero_grad()               # ê¸°ìš¸ê¸° ì´ˆê¸°í™” (ì´ì „ê°’ë“¤ì„ ì´ˆê¸°í™”í•˜ê³  í˜„ ê°€ì¤‘ì¹˜ ë•Œì˜ ê¸°ìš¸ê¸°ë§Œ ì‚¬ìš©)
                                        # ê° ë°°ì¹˜ë§ˆë‹¤ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
                                        # >>> ê¸°ìš¸ê¸° ëˆ„ì ì— ì˜í•œ ë¬¸ì œ í•´ê²°
                                        # ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œëŠ” í†µìƒì ìœ¼ë¡œ í•´ì•¼ì§€ ì„±ëŠ¥ ì•ˆì •ì 
                                        # >>> 
                                        # >>> ì•ˆí•˜ë©´ ê³¼ë„í•˜ê²Œ ê°’ì´ ì»¤ì§
                                        
    hypothesis = model(x)               # ê°€ì„¤ ì„¤ì • == ëª¨ë¸ ì„¤ì • : y =xw + b
                                        # hepothesis = y_prd
                                        
    loss = criterion(y, hypothesis)     # loss = mse() /// ì—¬ê¸°ê¹Œì§€ê°€ ìˆœì „íŒŒ
    
    loss.backward()                     # ì—­ì „íŒŒ(backward)ì‹œì‘ /// ê°€ì¤‘ì¹˜ ê°±ì‹ ì„ ìœ„í•´ì„œ ì§„í–‰í•˜ëŠ” ê¸°ìš¸ê¸°ê°’ê¹Œì§€ ê³„ì‚°
                                        # ê°€ì¤‘ì¹˜ ê°’ì´ ë‚˜ì˜¤ëŠ”ê±´ ì•„ë‹˜
                                        
    optimizer.step()                    # ê°€ì¤‘ì¹˜ ê°±ì‹  /// 1 epoch (batchë¥¼ ì„¤ì •í–ˆìœ¼ë©´ 1 batch)
    
    return loss.item()                  # torchtensor í˜•íƒœì˜ ìˆ˜ì¹˜ë¥¼ numpyë¡œ ì „í™˜

    # ê¸°ìš¸ê¸° ê³„ì‚°ì‹œ ì´í•´í•´ì•¼í•˜ëŠ” ê°œë… : ë¯¸ë¶„, í¸ë¯¸ë¶„, Chain-Rule

epochs = 2000

for epoch in range(1, epochs+1):
    loss = train(model, criterion, optimizer, x, y)
    print(f'epoch : {epoch:>3d} | loss : {loss}')
                
''' in tensorflow...
    model.compile(loss = 'mse',
                  optimizer = 'adam') '''

print('ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹')
############################################################
#4. í‰ê°€ ì˜ˆì¸¡
############################################################
''' in tensorflow...
    loss = model.evaluate(x, y) '''

def evaluate(model, criterion, x, y):
    model.eval()                        # [í‰ê°€ëª¨ë“œ] í‰ê°€ì— ëŒ€í•´ì„œëŠ” ë°˜ë“œì‹œ ë“¤ì–´ê°€ì•¼í•˜ëŠ” ê°’
                                        #            í›ˆë ¨ì—ì„œ ì§„í–‰í•œ DROPOUT, BATCHNORMAILATIONì„ ì ìš©í•˜ì§€ ì•ŠìŒ
                                        
    with torch.no_grad():               # gradient ê°±ì‹ ì„ í•˜ì§€ ì•Šê² ë‹¤
        y_prd = model(x)                # x_tst ê°’ìœ¼ë¡œ y_prd ì˜ˆì¸¡
        F_lss = criterion(y, y_prd)     # ìµœì¢… loss ê°’ ê³„ì‚°
    
    return F_lss.item()

n = 4
F_lss = evaluate(model, criterion, x, y)
x_prd = (torch.Tensor([[n]]).to(DEVICE) - mean) / std

reslt = model(x_prd)

print('Final_loss :', F_lss)
print(f'Predict{[n]} :', reslt.item())

#ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹
# Final_loss : 2.98394808861957e-10
# Predict[4] : 3.9999630451202393
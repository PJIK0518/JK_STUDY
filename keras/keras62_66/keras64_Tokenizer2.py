import numpy as np
import pandas as pd

from tensorflow.keras.preprocessing.text import Tokenizer

txt1 = '오늘도 못생기고 영어를 디게 디게 디게 못 하는 일삭이는 재미없는 \
        개그를 마구 마구 마구 마구하면서 딴짓을 한다.'
    
txt2 = '오늘도 박석사가 자아를 디게 디게 찾아냈다. \
       상진이는 마구 마구 딴짓을 한다. \
       재현은 못생기고 재미없는 딴짓을 한다.'
        
token = Tokenizer() 

token.fit_on_texts([txt1, txt2])

""" print(token.word_index)
{'디게': 1, '마구': 2, '딴짓을': 3, '한다': 4, '오늘도': 5, '못생기고': 6,
 '재미없는': 7, '영어를': 8, '못': 9, '하는': 10, '일삭이는': 11, '개그를': 12,
 '마구하면서': 13, '박석사가': 14, '자아를': 15, '찾아냈다': 16, '상진이는': 17, '재현은': 18} """
 
""" print(token.word_counts)
[('오늘도', 2), ('못생기고', 2), ('영어를', 1), ('디게', 5), ('못', 1), ('하는', 1),
 ('일삭이는', 1), ('재미없는', 2), ('개그를', 1), ('마구', 5), ('마구하면서', 1), ('딴짓을', 3),
 ('한다', 3), ('박석사가', 1), ('자아를', 1), ('찾아냈다', 1), ('상진이는', 1), ('재현은', 1)] """


x = token.texts_to_sequences([txt1,txt2])

""" print(x)
[[5, 6, 8, 1, 1, 1, 9, 10, 11, 7, 12, 2, 2, 2, 13, 3, 4],
 [5, 14, 15, 1, 1, 16, 17, 2, 2, 3, 4, 18, 6, 7, 3, 4]] """


x = np.concatenate(x)

""" print(x)
[ 5  6  8  1  1  1  9 10 11  7 12  2  2  2 13  3 
 4  5 14 15  1  1 16 17 2  2  3  4 18  6  7  3  4] """
 

############################
### OneHotEncoding
from sklearn.preprocessing import OneHotEncoder
x = np.array(x)
""" print(x)
[ 5  6  8  1  1  1  9 10 11  7 12  2  2  2 13  3
 4  5 14 15  1  1 16 17 2  2  3  4 18  6  7  3  4] """
""" print(x.shape) (33,) """

#1. pandas
# x = x - 1
# x = pd.get_dummies(x)

""" print(x)
print(x.shape)
    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17
0    0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
1    0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0
2    0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0
3    1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
4    1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
5    1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
6    0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0
7    0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0
8    0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0
9    0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0
10   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0
11   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
12   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
13   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
14   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0
15   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
16   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0
17   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
18   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0
19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0
20   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
21   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
22   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0
23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0
24   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
25   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
26   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
27   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0
28   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1
29   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0
30   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0
31   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
32   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0
(33, 18)
"""

#2. sklearn
# x = np.reshape(x, (-1,1))
# one = OneHotEncoder(sparse_output=False)
# x = one.fit_transform(x)

""" print(x)
print(x.shape)
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
(33, 18) """

#3. keras
from tensorflow.keras.utils import to_categorical

x = x - 1
x = to_categorical(x)

""" print(x)
print(x.shape)
[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
(33, 18) """

# Tokenizer : 단어에 번호를 매김 > 단어 마다 수치적 의미가 있는건 ㄴㄴㄴ
# OneHotEcd : 단어가 많아지면 0이 너무 많아지고 데이터가 비대해짐
# WordEmbed : 단어와 유사도 및 관련성으로 단어마다 좌표(벡터)를 부여 > 이미 만들어진 툴을 사용하면됨!